#!/usr/bin/env python3
"""
extract_mapping_csv.py

Extracts mapping CSVs from enriched d365-entities/ JSON files. Preserves
confirmed SF mapping columns (sfObjectName, sfFieldDisplayName, sfFieldApiName)
from existing CSVs. Clears sfSuggested* columns (regenerated by step 4).

Usage:
    python extract_mapping_csv.py account           # single entity
    python extract_mapping_csv.py --all             # all source JSONs
"""

import csv
import json
import os
import sys
import argparse

SCRIPTS_DIR = os.path.dirname(os.path.abspath(__file__))
PROJECT_DIR = os.path.dirname(SCRIPTS_DIR)
DEFAULT_SOURCE_DIR = os.path.join(PROJECT_DIR, "d365-entities")
DEFAULT_MAPPING_DIR = os.path.join(PROJECT_DIR, "mapping")

SF_CONFIRMED_COLUMNS = [
    "sfObjectName",
    "sfFieldDisplayName",
    "sfFieldApiName",
]

SF_SUGGESTED_COLUMNS = [
    "sfSuggestedObjectName",
    "sfSuggestedFieldDisplayName",
    "sfSuggestedFieldApiName",
]

SF_COLUMNS = SF_CONFIRMED_COLUMNS + SF_SUGGESTED_COLUMNS

D365_COLUMNS = ["displayName", "dataType", "requiredLevel", "isCustom"]
REPORT_COLUMNS = ["picklistValues", "mappingSuggested"]

# JSON section keys used to compute mappingSuggested
SECTION_KEYS = [
    "forms", "views", "chartVisualizations", "reports", "dashboards",
    "workflows", "javaScript", "formulas", "plugins", "pcfControls",
    "relationships", "ribbon",
]

# Reference count columns: CSV header -> JSON section key
REF_COUNT_COLUMNS = [
    ("refForms", "forms"),
    ("refViews", "views"),
    ("refChartVisualizations", "chartVisualizations"),
    ("refReports", "reports"),
    ("refDashboards", "dashboards"),
    ("refWorkflows", "workflows"),
    ("refFormulas", "formulas"),
    ("refPlugins", "plugins"),
    ("refPcfControls", "pcfControls"),
    ("refRelationships", "relationships"),
    ("refRibbon", "ribbon"),
]

CSV_COLUMNS = (D365_COLUMNS + REPORT_COLUMNS
               + [col for col, _ in REF_COUNT_COLUMNS] + SF_COLUMNS)


def load_existing_csv(csv_path):
    """Load confirmed SF columns from an existing mapping CSV, keyed by fieldName."""
    confirmed = {}
    if not os.path.isfile(csv_path):
        return confirmed
    with open(csv_path, "r", encoding="utf-8") as f:
        reader = csv.DictReader(f)
        for row in reader:
            field_name = row.get("fieldName", "")
            if field_name:
                confirmed[field_name] = {
                    col: row.get(col, "") for col in SF_CONFIRMED_COLUMNS
                }
    return confirmed


def extract_mapping(source_file, output_file):
    """Read a source JSON, preserve confirmed SF columns, write mapping CSV."""
    with open(source_file, "r", encoding="utf-8") as f:
        data = json.load(f)

    # Load confirmed SF mappings from existing CSV (if any)
    existing_sf = load_existing_csv(output_file)

    header = ["fieldName"] + CSV_COLUMNS
    rows = []
    for field in data.get("fields", []):
        field_name = field.get("fieldName", "")

        # Format picklist values
        pv = field.get("picklistValues")
        if isinstance(pv, list):
            pv_str = ', '.join(
                f'{v.get("value", "")}: {v.get("label", "")}' for v in pv
            ) if pv else ''
        else:
            pv_str = str(pv) if pv else ''

        # Compute mappingSuggested from section arrays
        has_usage = any(
            len(field.get(k) or []) > 0
            for k in SECTION_KEYS
        )
        required_level = (field.get("requiredLevel") or "").lower()
        is_required = required_level not in ("", "none")
        mapping_suggested = "true" if (has_usage or is_required) else "false"

        row = {
            "fieldName": field_name,
            "displayName": field.get("displayName", ""),
            "dataType": field.get("dataType", ""),
            "requiredLevel": field.get("requiredLevel", ""),
            "isCustom": str(field.get("isCustom", False)),
            "picklistValues": pv_str,
            "mappingSuggested": mapping_suggested,
        }

        # Reference counts from per-field section arrays
        for csv_col, json_key in REF_COUNT_COLUMNS:
            section = field.get(json_key)
            row[csv_col] = len(section) if isinstance(section, list) else 0

        # Preserve confirmed SF columns from existing CSV
        prev = existing_sf.get(field_name, {})
        for col in SF_CONFIRMED_COLUMNS:
            row[col] = prev.get(col, "")

        # Clear sfSuggested* columns (regenerated by step 4)
        for col in SF_SUGGESTED_COLUMNS:
            row[col] = ""

        rows.append(row)

    rows.sort(key=lambda r: r["fieldName"].lower())

    os.makedirs(os.path.dirname(output_file), exist_ok=True)
    with open(output_file, "w", encoding="utf-8", newline="") as f:
        writer = csv.DictWriter(f, fieldnames=header)
        writer.writeheader()
        writer.writerows(rows)

    return len(rows)


def main():
    parser = argparse.ArgumentParser(
        description="Extract Salesforce mapping data from source JSONs into CSV files"
    )
    parser.add_argument(
        "entity", nargs="?", default=None,
        help="Entity name (e.g., account, contact)"
    )
    parser.add_argument(
        "--all", action="store_true",
        help="Process all source JSON files"
    )
    parser.add_argument(
        "--source-dir", default=DEFAULT_SOURCE_DIR,
        help=f"Source JSON directory (default: {DEFAULT_SOURCE_DIR})"
    )
    parser.add_argument(
        "--mapping-dir", default=DEFAULT_MAPPING_DIR,
        help=f"Output CSV directory (default: {DEFAULT_MAPPING_DIR})"
    )
    args = parser.parse_args()

    if not args.all and not args.entity:
        parser.error("either provide an entity name or use --all")

    if not os.path.isdir(args.source_dir):
        print(f"ERROR: Source directory not found: {args.source_dir}", file=sys.stderr)
        sys.exit(1)

    print("=" * 60)
    print("Extract Salesforce Mapping CSVs")
    print("=" * 60)

    if args.all:
        json_files = sorted(f for f in os.listdir(args.source_dir) if f.endswith(".json"))
        entity_list = [os.path.splitext(f)[0] for f in json_files]
        print(f"Found {len(entity_list)} source JSON files.\n")
    else:
        entity_list = [args.entity.lower()]

    success = 0
    skipped = 0

    for entity_name in entity_list:
        source_file = os.path.join(args.source_dir, f"{entity_name}.json")
        if not os.path.isfile(source_file):
            print(f"  SKIP: {entity_name} â€” source JSON not found")
            skipped += 1
            continue

        output_file = os.path.join(args.mapping_dir, f"{entity_name}.csv")
        row_count = extract_mapping(source_file, output_file)
        print(f"  {entity_name}: {row_count} mapped fields -> {output_file}")
        success += 1

    print(f"\nDone: {success} CSVs written, {skipped} skipped.")


if __name__ == "__main__":
    main()
